{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BSQuestion_GenFIB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWsrbV66koBtLJ9snH6JSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bipulsin/ML-Projects/blob/main/BSQuestion_GenFIB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF7QBDiyu2MA",
        "outputId": "6dd771ed-cbc6-46cc-c931-7c3ae3caa211"
      },
      "source": [
        "!pip install gradio\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.1.4-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.7.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.1)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 39.9 MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.18.2-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.79.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 540 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Collecting starlette==0.19.1\n",
            "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.1)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 42.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 29.9 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=c0b2dab3554cdfb35ff236c5327466a54d48d7751451e2c69942b113871d058e\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=937ce1c22aeabfb91e4222ffa635830d21ac20588e5308258a3c7fa2b5c44315\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, monotonic, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, backoff, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, fsspec, ffmpy, fastapi, analytics-python, gradio\n",
            "Successfully installed analytics-python-1.4.0 anyio-3.6.1 backoff-1.10.0 bcrypt-3.2.2 cryptography-37.0.4 fastapi-0.79.0 ffmpy-0.3.0 fsspec-2022.7.1 gradio-3.1.4 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 orjson-3.7.11 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.2.0 starlette-0.19.1 uc-micro-py-1.0.1 uvicorn-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79yY9DPoORFs"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import Markdown, display, clear_output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVQaoLELOYJu"
      },
      "source": [
        "import _pickle as cPickle\n",
        "from pathlib import Path\n",
        "\n",
        "def dumpPickle(fileName, content):\n",
        "    pickleFile = open(fileName, 'wb')\n",
        "    cPickle.dump(content, pickleFile, -1)\n",
        "    pickleFile.close()\n",
        "\n",
        "def loadPickle(fileName):    \n",
        "    file = open(fileName, 'rb')\n",
        "    content = cPickle.load(file)\n",
        "    file.close()\n",
        "    \n",
        "    return content\n",
        "    \n",
        "def pickleExists(fileName):\n",
        "    file = Path(fileName)\n",
        "    \n",
        "    if file.is_file():\n",
        "        return True\n",
        "    \n",
        "    return False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvtBmRObOhk4"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Extract answers and the sentence they are in\n",
        "def extractAnswers(qas, doc):\n",
        "    answers = []\n",
        "\n",
        "    senStart = 0\n",
        "    senId = 0\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        senLen = len(sentence.text)\n",
        "\n",
        "        for answer in qas:\n",
        "            answerStart = answer['answers'][0]['answer_start']\n",
        "\n",
        "            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n",
        "                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n",
        "\n",
        "        senStart += senLen\n",
        "        senId += 1\n",
        "    \n",
        "    return answers\n",
        "\n",
        "#TODO - Clean answers from stopwords?\n",
        "def tokenIsAnswer(token, sentenceId, answers):\n",
        "    for i in range(len(answers)):\n",
        "        if (answers[i]['sentenceId'] == sentenceId):\n",
        "            if (answers[i]['text'] == token):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "#Save named entities start points\n",
        "\n",
        "def getNEStartIndexs(doc):\n",
        "    neStarts = {}\n",
        "    for ne in doc.ents:\n",
        "        neStarts[ne.start] = ne\n",
        "        \n",
        "    return neStarts \n",
        "\n",
        "def getSentenceStartIndexes(doc):\n",
        "    senStarts = []\n",
        "    \n",
        "    for sentence in doc.sents:\n",
        "        senStarts.append(sentence[0].i)\n",
        "    \n",
        "    return senStarts\n",
        "    \n",
        "def getSentenceForWordPosition(wordPos, senStarts):\n",
        "    for i in range(1, len(senStarts)):\n",
        "        if (wordPos < senStarts[i]):\n",
        "            return i - 1\n",
        "        \n",
        "def addWordsForParagrapgh(newWords, text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    neStarts = getNEStartIndexs(doc)\n",
        "    senStarts = getSentenceStartIndexes(doc)\n",
        "    \n",
        "    #index of word in spacy doc text\n",
        "    i = 0\n",
        "    \n",
        "    while (i < len(doc)):\n",
        "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
        "        if (i in neStarts):\n",
        "            word = neStarts[i]\n",
        "            #add word\n",
        "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
        "            wordLen = word.end - word.start\n",
        "            shape = ''\n",
        "            for wordIndex in range(word.start, word.end):\n",
        "                shape += (' ' + doc[wordIndex].shape_)\n",
        "\n",
        "            newWords.append([word.text,\n",
        "                            0,\n",
        "                            0,\n",
        "                            currentSentence,\n",
        "                            wordLen,\n",
        "                            word.label_,\n",
        "                            None,\n",
        "                            None,\n",
        "                            None,\n",
        "                            shape])\n",
        "            i = neStarts[i].end - 1\n",
        "        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n",
        "        else:\n",
        "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
        "                word = doc[i]\n",
        "\n",
        "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
        "                wordLen = 1\n",
        "\n",
        "                newWords.append([word.text,\n",
        "                                0,\n",
        "                                0,\n",
        "                                currentSentence,\n",
        "                                wordLen,\n",
        "                                None,\n",
        "                                word.pos_,\n",
        "                                word.tag_,\n",
        "                                word.dep_,\n",
        "                                word.shape_])\n",
        "        i += 1\n",
        "\n",
        "def oneHotEncodeColumns(df):\n",
        "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
        "\n",
        "    for column in columnsToEncode:\n",
        "        one_hot = pd.get_dummies(df[column])\n",
        "        one_hot = one_hot.add_prefix(column + '_')\n",
        "\n",
        "        df = df.drop(column, axis = 1)\n",
        "        df = df.join(one_hot)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQipf2mYOqKn"
      },
      "source": [
        "def generateDf(text):\n",
        "    words = []\n",
        "    addWordsForParagrapgh(words, text)\n",
        "\n",
        "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
        "    df = pd.DataFrame(words, columns=wordColums)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptXHC3HOOusb"
      },
      "source": [
        "def prepareDf(df):\n",
        "    #One-hot encoding\n",
        "    wordsDf = oneHotEncodeColumns(df)\n",
        "\n",
        "    #Drop unused columns\n",
        "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape']\n",
        "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
        "\n",
        "    #Add missing colums \n",
        "    predictorColumns = ['wordCount','NER_CARDINAL','NER_DATE','NER_EVENT','NER_FAC','NER_GPE','NER_LANGUAGE','NER_LAW','NER_LOC','NER_MONEY','NER_NORP','NER_ORDINAL','NER_ORG','NER_PERCENT','NER_PERSON','NER_PRODUCT','NER_QUANTITY','NER_TIME','NER_WORK_OF_ART','POS_ADJ','POS_ADP','POS_ADV','POS_CCONJ','POS_DET','POS_INTJ','POS_NOUN','POS_NUM','POS_PART','POS_PRON','POS_PROPN','POS_PUNCT','POS_SYM','POS_VERB','POS_X','TAG_''','TAG_-LRB-','TAG_.','TAG_ADD','TAG_AFX','TAG_CC','TAG_CD','TAG_DT','TAG_EX','TAG_FW','TAG_IN','TAG_JJ','TAG_JJR','TAG_JJS','TAG_LS','TAG_MD','TAG_NFP','TAG_NN','TAG_NNP','TAG_NNPS','TAG_NNS','TAG_PDT','TAG_POS','TAG_PRP','TAG_PRP$','TAG_RB','TAG_RBR','TAG_RBS','TAG_RP','TAG_SYM','TAG_TO','TAG_UH','TAG_VB','TAG_VBD','TAG_VBG','TAG_VBN','TAG_VBP','TAG_VBZ','TAG_WDT','TAG_WP','TAG_WRB','TAG_XX','DEP_ROOT','DEP_acl','DEP_acomp','DEP_advcl','DEP_advmod','DEP_agent','DEP_amod','DEP_appos','DEP_attr','DEP_aux','DEP_auxpass','DEP_case','DEP_cc','DEP_ccomp','DEP_compound','DEP_conj','DEP_csubj','DEP_csubjpass','DEP_dative','DEP_dep','DEP_det','DEP_dobj','DEP_expl','DEP_intj','DEP_mark','DEP_meta','DEP_neg','DEP_nmod','DEP_npadvmod','DEP_nsubj','DEP_nsubjpass','DEP_nummod','DEP_oprd','DEP_parataxis','DEP_pcomp','DEP_pobj','DEP_poss','DEP_preconj','DEP_predet','DEP_prep','DEP_prt','DEP_punct','DEP_quantmod','DEP_relcl','DEP_xcomp']\n",
        "\n",
        "    for feature in predictorColumns:\n",
        "        if feature not in wordsDf.columns:\n",
        "            wordsDf[feature] = 0\n",
        "    \n",
        "    return wordsDf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0TzI3SwO0lc"
      },
      "source": [
        "def predictWords(wordsDf, df):\n",
        "    \n",
        "    predictorPickleName = '/content/sample_data/nb-predictor.pkl'\n",
        "    predictor = loadPickle(predictorPickleName)\n",
        "    \n",
        "    y_pred = predictor.predict_proba(wordsDf)\n",
        "\n",
        "    labeledAnswers = []\n",
        "    for i in range(len(y_pred)):\n",
        "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
        "    \n",
        "    return labeledAnswers"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W5mKbXOO7xt"
      },
      "source": [
        "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
        "    leftPartStart = doc[sentStart].idx\n",
        "    leftPartEnd = doc[firstTokenIndex].idx\n",
        "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
        "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
        "    \n",
        "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
        "    \n",
        "    return question"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7YSpPJQPAWo"
      },
      "source": [
        "def addQuestions(answers, text):\n",
        "    doc = nlp(text)\n",
        "    currAnswerIndex = 0\n",
        "    qaPair = []\n",
        "\n",
        "    #Check wheter each token is the next answer\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            \n",
        "            #If all the answers have been found, stop looking\n",
        "            if currAnswerIndex >= len(answers):\n",
        "                break\n",
        "            \n",
        "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
        "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
        "            answerIsFound = True\n",
        "            \n",
        "            for j in range(len(answerDoc)):\n",
        "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
        "                    answerIsFound = False\n",
        "           \n",
        "            #If the current token is corresponding with the answer, add it \n",
        "            if answerIsFound:\n",
        "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
        "                \n",
        "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
        "                \n",
        "                currAnswerIndex += 1\n",
        "                \n",
        "    return qaPair"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFtueGwoPF35"
      },
      "source": [
        "\n",
        "def sortAnswers(qaPairs):\n",
        "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
        "    \n",
        "    return orderedQaPairs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODt_lCD2PKnX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "bbd4e9b9-9b54-40c2-a4c0-64ab9c863e18"
      },
      "source": [
        "import gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "\n",
        "tmp_file = get_tmpfile('word2vec.6B.300d.txt')\n",
        "glove2word2vec('/content/sample_data/glove.6B.300d.txt', tmp_file)\n",
        "\n",
        "#glove2word2vec(glove_file, tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-51c7c62dbf5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtmp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec.6B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/glove.6B.300d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#glove2word2vec(glove_file, tmp_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[0;34m(glove_file_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/glove.6B.300d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuS_re2wB-m"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hKq-kN9grEe"
      },
      "source": [
        "def generate_distractors(answer, count):\n",
        "    answer = str.lower(answer)\n",
        "    \n",
        "    ##Extracting closest words for the answer. \n",
        "    try:\n",
        "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
        "    except:\n",
        "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
        "        return []\n",
        "\n",
        "    #Return count many distractors\n",
        "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
        "    \n",
        "    return distractors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GZYNGPYhBQ7"
      },
      "source": [
        "def addDistractors(qaPairs, count):\n",
        "    for qaPair in qaPairs:\n",
        "        distractors = generate_distractors(qaPair['answer'], count)\n",
        "        qaPair['distractors'] = distractors\n",
        "    \n",
        "    return qaPairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z088WZtQhGeK"
      },
      "source": [
        "def generateQuestions(text, count):\n",
        "    \n",
        "    # Extract words \n",
        "    df = generateDf(text)\n",
        "    wordsDf = prepareDf(df)\n",
        "    \n",
        "    # Predict \n",
        "    labeledAnswers = predictWords(wordsDf, df)\n",
        "    \n",
        "    # Transform questions\n",
        "    qaPairs = addQuestions(labeledAnswers, text)\n",
        "    \n",
        "    # Pick the best questions\n",
        "    orderedQaPairs = sortAnswers(qaPairs)\n",
        "    \n",
        "    # Generate distractors\n",
        "    questions = addDistractors(orderedQaPairs[:count], 4)\n",
        "\n",
        "   \n",
        "    # Print\n",
        "    for i in range(count):\n",
        "        display(Markdown('### Question ' + str(i + 1) + ':'))\n",
        "        print(questions[i]['question'])\n",
        "\n",
        "        display(Markdown('#### Answer:'))\n",
        "        print(questions[i]['answer'])\n",
        "        \n",
        "        display(Markdown('#### Incorrect answers:'))\n",
        "        for distractor in questions[i]['distractors']:\n",
        "            print(distractor)\n",
        "        \n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUKb2TighVFJ"
      },
      "source": [
        "#text = \"The longest river in Africa, it has historically been considered the longest river in the world, though this has been contested by research suggesting that the Amazon River is slightly longer. The Nile is about 6650 km long and its drainage basin covers eleven countries:  Tanzania, Uganda, Rwanda, Burundi, the Democratic Republic of the Congo, Kenya, Ethiopia, Eritrea, South Sudan, Republic of the Sudan, and Egypt. In particular, the Nile is the primary water source of Egypt and Sudan. The Nile has two major tributaries – the White Nile and the Blue Nile. The White Nile is considered to be the headwaters and primary stream of the Nile itself. The Blue Nile, however, is the source of most of the water, containing 80% of the water and silt. The White Nile is longer and rises in the Great Lakes region of central Africa, with the most distant source still undetermined but located in either Rwanda or Burundi. It flows north through Tanzania, Lake Victoria, Uganda and South Sudan. The Blue Nile begins at Lake Tana in Ethiopia and flows into Sudan from the southeast. The two rivers meet just north of the Sudanese capital of Khartoum.\"\n",
        "\n",
        "#generateQuestions(text, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaIZy5xaq9kD"
      },
      "source": [
        "import gradio as gr\n",
        "gr.Interface(generateQuestions,\n",
        "    [\n",
        "        gr.inputs.Textbox(lines=7, label=\"Context\"),\n",
        "        gr.inputs.Textbox(label=\"Count\"),\n",
        "    ],\n",
        "    gr.outputs.Textbox(label=\"Answer\")).launch()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlydAgLSS-u"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}